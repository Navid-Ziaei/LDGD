{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from LDGD.model import ARDRBFKernel, LDGD\n",
    "from LDGD.model.experimental.GP_scratch import bGPLVM\n",
    "from LDGD.visualization import plot_box_plots\n",
    "from gpytorch.likelihoods import GaussianLikelihood, BernoulliLikelihood\n",
    "from gpytorch.mlls import VariationalELBO\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.io import savemat, loadmat\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:57:59.492203Z",
     "start_time": "2024-02-27T05:57:55.996417400Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:57:59.617167700Z",
     "start_time": "2024-02-27T05:57:59.492203Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "model_settings = {\n",
    "    'latent_dim': 2,\n",
    "    'num_inducing_points': 5,\n",
    "    'num_epochs_train': 5000,\n",
    "    'num_epochs_test': 5000,\n",
    "    'batch_size': 200,\n",
    "    'load_trained_model': False,\n",
    "    'load_tested_model': False,\n",
    "    'use_gpytorch': True,\n",
    "    'n_features': 10\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:57:59.728286900Z",
     "start_time": "2024-02-27T05:57:59.617167700Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Oil dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_data_oil = np.load('../data/train_data.npy', allow_pickle=True)\n",
    "test_data_oil = np.load('../data/test_data.npy', allow_pickle=True)\n",
    "\n",
    "yn_train_oil, ys_train_oil, labels_train_oil = train_data_oil.take(0)['yn_train'], train_data_oil.take(0)['ys_train'], train_data_oil.take(0)['labels_train']\n",
    "yn_test_oil, ys_test_oil, labels_test_oil = test_data_oil.take(0)['yn_test'], test_data_oil.take(0)['ys_test'], test_data_oil.take(0)['labels_test']\n",
    "\n",
    "yn_train_oil, ys_train_oil, labels_train_oil = torch.Tensor(yn_train_oil), torch.Tensor(ys_train_oil), torch.Tensor(labels_train_oil)\n",
    "yn_test_oil, ys_test_oil, labels_test_oil = torch.Tensor(yn_test_oil), torch.Tensor(ys_test_oil), torch.Tensor(labels_test_oil)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:57:59.837668600Z",
     "start_time": "2024-02-27T05:57:59.728286900Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "plot_box_plots(data=yn_train_oil, labels=labels_train_oil, save_path='saved_results/', file_name='box_plot_oil_flow')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:02.752729800Z",
     "start_time": "2024-02-27T05:57:59.837668600Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load IRIS dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_data_iris = np.load('../data/train_data_iris.npy', allow_pickle=True)\n",
    "test_data_iris = np.load('../data/test_data_iris.npy', allow_pickle=True)\n",
    "\n",
    "yn_train_iris, ys_train_iris, labels_train_iris = train_data_iris.take(0)['yn_train'], train_data_iris.take(0)['ys_train'], train_data_iris.take(0)['labels_train']\n",
    "yn_test_iris, ys_test_iris, labels_test_iris = test_data_iris.take(0)['yn_test'], test_data_iris.take(0)['ys_test'], test_data_iris.take(0)['labels_test']\n",
    "\n",
    "yn_train_iris, ys_train_iris, labels_train_iris = torch.Tensor(yn_train_iris), torch.Tensor(ys_train_iris), torch.Tensor(labels_train_iris)\n",
    "yn_test_iris, ys_test_iris, labels_test_iris = torch.Tensor(yn_test_iris), torch.Tensor(ys_test_iris), torch.Tensor(labels_test_iris)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:03.143694400Z",
     "start_time": "2024-02-27T05:58:02.752729800Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "plot_box_plots(data=yn_train_iris, labels=labels_train_iris, save_path='saved_results/', file_name='box_plot_iris')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.080974600Z",
     "start_time": "2024-02-27T05:58:02.862405200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.080974600Z",
     "start_time": "2024-02-27T05:58:03.706895600Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "X_pca_oil = PCA(n_components=2).fit_transform(yn_train_oil)\n",
    "X_pca_iris = PCA(n_components=2).fit_transform(yn_train_iris)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.080974600Z",
     "start_time": "2024-02-27T05:58:03.706895600Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# T-SNE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "try:\n",
    "    x_tsne_oil = np.load('../data/x_tsne_oil.npy')\n",
    "except:\n",
    "    x_tsne_oil = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3).fit_transform(yn_train_oil)\n",
    "    np.save('../data/x_tsne_iris.npy', x_tsne_oil)\n",
    "\n",
    "try:\n",
    "    x_tsne_iris = np.load('../data/x_tsne_iris.npy')\n",
    "except:\n",
    "    x_tsne_iris = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3).fit_transform(yn_train_iris)\n",
    "    np.save('../data/x_tsne_iris.npy', x_tsne_iris)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.355999500Z",
     "start_time": "2024-02-27T05:58:03.831543200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GPLVM - Point Estimate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "try:\n",
    "    x_param_gplvm_iris = np.load('../data/x_param_gplvm_iris.npy')\n",
    "except:\n",
    "    num_points, data_dim = yn_train_iris.shape\n",
    "    model = bGPLVM(num_points, data_dim, model_settings['latent_dim'], model_settings['num_inducing_points'], mode='point_estimate')\n",
    "\n",
    "    # Likelihood\n",
    "    likelihood = GaussianLikelihood(batch_shape=model.batch_shape)\n",
    "    mll = VariationalELBO(likelihood, model, num_data=len(yn_train_iris))\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()}], lr=0.01)\n",
    "\n",
    "    loss_list = model.train_model(yn_train_iris, optimizer, mll, epochs=3000)\n",
    "\n",
    "    x_param_gplvm_iris = model.X.X.detach().numpy()\n",
    "    np.save('../data/x_param_gplvm_iris.npy', x_param_gplvm_iris)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.355999500Z",
     "start_time": "2024-02-27T05:58:04.051098500Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "try:\n",
    "    x_param_gplvm_oil = np.load('../data/x_param_gplvm_oil.npy')\n",
    "except:\n",
    "    num_points, data_dim = yn_train_oil.shape\n",
    "    model = bGPLVM(num_points, data_dim, model_settings['latent_dim'], model_settings['num_inducing_points'], mode='point_estimate')\n",
    "\n",
    "    # Likelihood\n",
    "    likelihood = GaussianLikelihood(batch_shape=model.batch_shape)\n",
    "    mll = VariationalELBO(likelihood, model, num_data=len(yn_train_oil))\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()}], lr=0.01)\n",
    "\n",
    "    loss_list = model.train_model(yn_train_oil, optimizer, mll, epochs=3000)\n",
    "\n",
    "    x_param_gplvm_oil = model.X.X.detach().numpy()\n",
    "    np.save('../data/x_param_gplvm_oil.npy', x_param_gplvm_oil)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.355999500Z",
     "start_time": "2024-02-27T05:58:04.160472900Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baysian GPLVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "try:\n",
    "    x_bayesian_gplvm_oil = np.load('../data/x_bayesian_gplvm_oil.npy')\n",
    "except:\n",
    "    num_points, data_dim = yn_train_oil.shape\n",
    "    model = bGPLVM(num_points, data_dim, model_settings['latent_dim'], model_settings['num_inducing_points'], mode='bayesian')\n",
    "\n",
    "    # Likelihood\n",
    "    likelihood = GaussianLikelihood(batch_shape=model.batch_shape)\n",
    "    mll = VariationalELBO(likelihood, model, num_data=len(yn_train_oil))\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()}], lr=0.01)\n",
    "\n",
    "    loss_list = model.train_model(yn_train_oil, optimizer, mll, epochs=5000)\n",
    "\n",
    "    # vISUALIZATION\n",
    "    x_bayesian_gplvm_oil = model.X.q_mu.detach().numpy()\n",
    "    np.save('../data/x_bayesian_gplvm_oil.npy', x_bayesian_gplvm_oil)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.373027800Z",
     "start_time": "2024-02-27T05:58:04.301097200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "try:\n",
    "    x_bayesian_gplvm_iris = np.load('../data/x_bayesian_gplvm_iris.npy')\n",
    "except:\n",
    "    num_points, data_dim = yn_train_iris.shape\n",
    "    model = bGPLVM(num_points, data_dim, model_settings['latent_dim'], model_settings['num_inducing_points'], mode='bayesian')\n",
    "\n",
    "    # Likelihood\n",
    "    likelihood = GaussianLikelihood(batch_shape=model.batch_shape)\n",
    "    mll = VariationalELBO(likelihood, model, num_data=len(yn_train_iris))\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()}], lr=0.01)\n",
    "\n",
    "    loss_list = model.train_model(yn_train_iris, optimizer, mll, epochs=5000)\n",
    "\n",
    "    # vISUALIZATION\n",
    "    x_bayesian_gplvm_iris = model.X.q_mu.detach().numpy()\n",
    "    np.save('../data/x_bayesian_gplvm_iris.npy', x_bayesian_gplvm_iris)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.373027800Z",
     "start_time": "2024-02-27T05:58:04.394849Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Shared GPLVM (DGPLVM)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "results_sgplvm_oil = loadmat(\"../data/result_sgplvm_oil.mat\")\n",
    "x_sgplvm_oil = results_sgplvm_oil['latent_z']\n",
    "\n",
    "results_sgplvm_iris = loadmat(\"../data/result_sgplvm_iris.mat\")\n",
    "x_sgplvm_iris = results_sgplvm_iris['latent_z']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.373027800Z",
     "start_time": "2024-02-27T05:58:04.535504800Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SLLGPLVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "results_sllgplvm_oil = loadmat(\"../data/result_sllgplvm.mat\")\n",
    "x_sllgplvm_oil = results_sllgplvm_oil['zz']\n",
    "\n",
    "results_sllgplvm_iris = loadmat(\"../data/result_sllgplvm_iris.mat\")\n",
    "x_sllgplvm_iris = results_sllgplvm_iris['z']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.387535800Z",
     "start_time": "2024-02-27T05:58:04.691722300Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fast GPLVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "results_fgplvm_oil = loadmat(\"../data/result_fgplvm.mat\")\n",
    "x_fgplvm_oil = results_fgplvm_oil['z']\n",
    "\n",
    "results_fgplvm_iris = loadmat(\"../data/result_fgplvm_iris.mat\")\n",
    "x_fgplvm_iris = results_fgplvm_iris['z']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.403201200Z",
     "start_time": "2024-02-27T05:58:04.785504100Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LDGD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "try:\n",
    "    x_ldgd_oil = np.load('../data/x_dbgplvm_oil.npy')\n",
    "except:\n",
    "    num_points, data_dim = yn_train_oil.shape\n",
    "    batch_shape = torch.Size([data_dim])\n",
    "    model_settings['use_gpytorch'] = True\n",
    "    if model_settings['use_gpytorch'] is False:\n",
    "        kernel_cls = ARDRBFKernel(input_dim=model_settings['latent_dim'])\n",
    "        kernel_reg = ARDRBFKernel(input_dim=model_settings['latent_dim'])\n",
    "    else:\n",
    "        kernel_reg = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=model_settings['latent_dim']))\n",
    "        kernel_cls = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=model_settings['latent_dim']))\n",
    "\n",
    "    likelihood_reg = GaussianLikelihood(batch_shape=batch_shape)\n",
    "    likelihood_cls = BernoulliLikelihood()\n",
    "\n",
    "    model = LDGD(yn_train_oil,\n",
    "                kernel_reg=kernel_reg,\n",
    "                kernel_cls=kernel_cls,\n",
    "                num_classes=ys_train_oil.shape[-1],\n",
    "                latent_dim=model_settings['latent_dim'],\n",
    "                num_inducing_points=model_settings['num_inducing_points'],\n",
    "                likelihood_reg=likelihood_reg,\n",
    "                likelihood_cls=likelihood_cls,\n",
    "                use_gpytorch=model_settings['use_gpytorch'],\n",
    "                shared_inducing_points=True,\n",
    "                use_shared_kernel=False)\n",
    "\n",
    "    losses, x_mu_list, x_sigma_list = model.train_model(yn=yn_train_oil,\n",
    "                                                        ys=ys_train_oil,\n",
    "                                                        epochs=model_settings['num_epochs_train'],\n",
    "                                                        batch_size=model_settings['batch_size'])\n",
    "\n",
    "    x_ldgd_oil = model.x.q_mu.detach().numpy()\n",
    "    np.save('../data/x_ldgd_oil.npy', x_ldgd_oil)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.403201200Z",
     "start_time": "2024-02-27T05:58:04.913658200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "try:\n",
    "    x_ldgd_iris = np.load('../data/x_dbgplvm_iris.npy')\n",
    "except:\n",
    "    num_points, data_dim = yn_train_iris.shape\n",
    "    batch_shape = torch.Size([data_dim])\n",
    "    model_settings['use_gpytorch'] = True\n",
    "    if model_settings['use_gpytorch'] is False:\n",
    "        kernel_cls = ARDRBFKernel(input_dim=model_settings['latent_dim'])\n",
    "        kernel_reg = ARDRBFKernel(input_dim=model_settings['latent_dim'])\n",
    "    else:\n",
    "        kernel_reg = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=model_settings['latent_dim']))\n",
    "        kernel_cls = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=model_settings['latent_dim']))\n",
    "\n",
    "    likelihood_reg = GaussianLikelihood(batch_shape=batch_shape)\n",
    "    likelihood_cls = BernoulliLikelihood()\n",
    "\n",
    "    model = LDGD(yn_train_iris,\n",
    "                kernel_reg=kernel_reg,\n",
    "                kernel_cls=kernel_cls,\n",
    "                num_classes=ys_train_iris.shape[-1],\n",
    "                latent_dim=model_settings['latent_dim'],\n",
    "                num_inducing_points=model_settings['num_inducing_points'],\n",
    "                likelihood_reg=likelihood_reg,\n",
    "                likelihood_cls=likelihood_cls,\n",
    "                use_gpytorch=model_settings['use_gpytorch'],\n",
    "                shared_inducing_points=True,\n",
    "                use_shared_kernel=False)\n",
    "\n",
    "    losses, x_mu_list, x_sigma_list = model.train_model(yn=yn_train_iris,\n",
    "                                                        ys=ys_train_iris,\n",
    "                                                        epochs=model_settings['num_epochs_train'],\n",
    "                                                        batch_size=model_settings['batch_size'])\n",
    "\n",
    "    x_ldgd_iris = model.x.q_mu.detach().numpy()\n",
    "    np.save('../data/x_ldgd_iris.npy', x_ldgd_iris)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.434420300Z",
     "start_time": "2024-02-27T05:58:05.054279100Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 8, figsize=(50, 16))\n",
    "fontsize = 28  # Variable for consistent font size\n",
    "\n",
    "color_list = ['r', 'b', 'g']\n",
    "label_list = ['class 1', 'class 2', 'class 3']  # Assuming there's a typo in your original label list\n",
    "titles_list = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"]\n",
    "\n",
    "\n",
    "# Plotting data\n",
    "for i in range(3):\n",
    "    ax[0, 0].scatter(X_pca_oil[labels_train_oil == i, 0], X_pca_oil[labels_train_oil == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[0, 1].scatter(x_tsne_oil[labels_train_oil == i, 0], x_tsne_oil[labels_train_oil == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[0, 2].scatter(x_param_gplvm_oil[labels_train_oil == i, 0], x_param_gplvm_oil[labels_train_oil == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[0, 3].scatter(x_bayesian_gplvm_oil[labels_train_oil == i, 0], x_bayesian_gplvm_oil[labels_train_oil == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[0, 4].scatter(x_fgplvm_oil[labels_train_oil == i, 0], x_fgplvm_oil[labels_train_oil == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[0, 5].scatter(x_sgplvm_oil[labels_train_oil == i, 0], x_sgplvm_oil[labels_train_oil == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[0, 5].set_xlim([np.quantile(x_sgplvm_oil[labels_train_oil == i, 0], 0.02),\n",
    "                       np.quantile(x_sgplvm_oil[labels_train_oil == i, 0], 0.98)])\n",
    "    ax[0, 5].set_ylim([np.quantile(x_sgplvm_oil[labels_train_oil == i, 1], 0.02),\n",
    "                       np.quantile(x_sgplvm_oil[labels_train_oil == i, 1], 0.98)])\n",
    "    ax[0, 6].scatter(x_sllgplvm_oil[labels_train_oil == i, 0], x_sllgplvm_oil[labels_train_oil == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[0, 7].scatter(x_ldgd_oil[labels_train_oil == i, 0], x_ldgd_oil[labels_train_oil == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "\n",
    "    # Iris\n",
    "    ax[1, 0].scatter(X_pca_iris[labels_train_iris == i, 0], X_pca_iris[labels_train_iris == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[1, 1].scatter(x_tsne_iris[labels_train_iris == i, 0], x_tsne_iris[labels_train_iris == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[1, 2].scatter(x_param_gplvm_iris[labels_train_iris == i, 0], x_param_gplvm_iris[labels_train_iris == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[1, 3].scatter(x_bayesian_gplvm_iris[labels_train_iris == i, 0], x_bayesian_gplvm_iris[labels_train_iris == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[1, 4].scatter(x_fgplvm_iris[labels_train_iris == i, 0], x_fgplvm_iris[labels_train_iris == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[1, 5].scatter(x_sgplvm_iris[labels_train_iris == i, 0], x_sgplvm_iris[labels_train_iris == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[1, 5].set_xlim([np.quantile(x_sgplvm_iris[labels_train_iris == i, 0], 0.02),\n",
    "                       np.quantile(x_sgplvm_iris[labels_train_iris == i, 0], 0.98)])\n",
    "    ax[1, 5].set_ylim([np.quantile(x_sgplvm_iris[labels_train_iris == i, 1], 0.02),\n",
    "                       np.quantile(x_sgplvm_iris[labels_train_iris == i, 1], 0.98)])\n",
    "    ax[1, 6].scatter(x_sllgplvm_iris[labels_train_iris == i, 0], x_sllgplvm_iris[labels_train_iris == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "    ax[1, 7].scatter(x_ldgd_iris[labels_train_iris == i, 0], x_ldgd_iris[labels_train_iris == i, 1], c=color_list[i],\n",
    "                     s=40, alpha=1, edgecolor=color_list[i])\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    ax[0, i].set_xlabel('$X_1$', fontsize=fontsize)\n",
    "    ax[0, i].set_ylabel('$X_2$', fontsize=fontsize)\n",
    "    ax[0, i].tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "\n",
    "    ax[0, i].spines['top'].set_visible(False)\n",
    "    ax[0, i].spines['right'].set_visible(False)\n",
    "\n",
    "    ax[0, i].set_title(titles_list[i], fontsize=34)\n",
    "    ax[1, i].set_xlabel('$X_1$', fontsize=fontsize)\n",
    "    ax[1, i].set_ylabel('$X_2$', fontsize=fontsize)\n",
    "    ax[1, i].tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "\n",
    "    ax[1, i].spines['top'].set_visible(False)\n",
    "    ax[1, i].spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"./saved_results/figure3.png\")\n",
    "fig.savefig(\"./saved_results/figure3.svg\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T05:58:10.620671Z",
     "start_time": "2024-02-27T05:58:05.282722200Z"
    }
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
