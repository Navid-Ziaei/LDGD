{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-29T01:42:26.807248500Z",
     "start_time": "2024-02-29T01:42:24.567276500Z"
    }
   },
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import winsound\n",
    "from matplotlib import pyplot as plt\n",
    "from LDGD.model import LDGD, FastLDGD, VAE\n",
    "from LDGD.visualization.vizualize_utils import plot_heatmap, plot_2d_scatter, plot_ARD_gplvm\n",
    "from LDGD.visualization.vizualize_utils import plot_loss_gplvm, plot_scatter_gplvm\n",
    "from gpytorch.likelihoods import GaussianLikelihood, BernoulliLikelihood\n",
    "\n",
    "import json\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def create_dataset(random_state, test_size, dataset='mnist', **kwargs):\n",
    "    if dataset == 'mnist':\n",
    "        mnist_train = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "        mnist_test = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "        # Flatten the images and convert labels\n",
    "        x_train = mnist_train.data.view(mnist_train.data.size(0), -1).numpy()\n",
    "        x_train = x_train/x_train.max()\n",
    "        y_train = mnist_train.targets.numpy()\n",
    "\n",
    "        # Concatenate train and test sets to split them later\n",
    "        x_test = mnist_test.data.view(mnist_test.data.size(0), -1).numpy()\n",
    "        x_test = x_test/x_test.max()\n",
    "        y_test = mnist_test.targets.numpy()\n",
    "\n",
    "        # One-hot encode the labels\n",
    "        y_one_hot_train = np.zeros((y_train.shape[0], len(np.unique(y_train))))\n",
    "        y_one_hot_train[np.arange(y_train.shape[0]), y_train] = 1\n",
    "\n",
    "        y_one_hot_test = np.zeros((y_test.shape[0], len(np.unique(y_test))))\n",
    "        y_one_hot_test[np.arange(y_test.shape[0]), y_test] = 1\n",
    "\n",
    "        orig_data = None  # No original data in the case of MNIST\n",
    "\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_one_hot_train, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_one_hot_test, dtype=torch.float32)\n",
    "    y_train_labels_tensor = torch.tensor(y_train)\n",
    "    y_test_labels_tensor = torch.tensor(y_test)\n",
    "\n",
    "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, y_train_labels_tensor, y_test_labels_tensor, orig_data\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T01:42:26.948418700Z",
     "start_time": "2024-02-29T01:42:26.807248500Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def create_LDGD_model(data_cont, data_cat, ldgd_settings, batch_shape, x_init='pca'):\n",
    "    if ldgd_settings['use_gpytorch'] is False:\n",
    "        pass\n",
    "    else:\n",
    "        kernel_reg = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=ldgd_settings['latent_dim']))\n",
    "        kernel_cls = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=ldgd_settings['latent_dim']))\n",
    "\n",
    "    likelihood_reg = GaussianLikelihood(batch_shape=batch_shape)\n",
    "    likelihood_cls = BernoulliLikelihood()\n",
    "    model = LDGD(data_cont,\n",
    "             kernel_reg=kernel_reg,\n",
    "             kernel_cls=kernel_cls,\n",
    "             num_classes=data_cat.shape[-1],\n",
    "             latent_dim=ldgd_settings['latent_dim'],\n",
    "             num_inducing_points_cls= ldgd_settings['num_inducing_points_cls'],\n",
    "             num_inducing_points_reg= ldgd_settings['num_inducing_points_reg'],\n",
    "             likelihood_reg=likelihood_reg,\n",
    "             likelihood_cls=likelihood_cls,\n",
    "             use_gpytorch=ldgd_settings['use_gpytorch'],\n",
    "             shared_inducing_points=ldgd_settings['shared_inducing_points'],\n",
    "             use_shared_kernel=False,\n",
    "             x_init=x_init,\n",
    "             device=device)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_FastLDGD_model(data_cont, data_cat, batch_shape, ldgd_settings):\n",
    "    kernel_reg = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=ldgd_settings['latent_dim']))\n",
    "    kernel_cls = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=ldgd_settings['latent_dim']))\n",
    "\n",
    "    likelihood_reg = GaussianLikelihood(batch_shape=batch_shape)\n",
    "    likelihood_cls = BernoulliLikelihood()\n",
    "    model = FastLDGD(data_cont,\n",
    "             kernel_reg=kernel_reg,\n",
    "             kernel_cls=kernel_cls,\n",
    "             num_classes=data_cat.shape[-1],\n",
    "             latent_dim=ldgd_settings['latent_dim'],\n",
    "             num_inducing_points_cls= ldgd_settings['num_inducing_points_cls'],\n",
    "             num_inducing_points_reg= ldgd_settings['num_inducing_points_reg'],\n",
    "             likelihood_reg=likelihood_reg,\n",
    "             likelihood_cls=likelihood_cls,\n",
    "             use_gpytorch=ldgd_settings['use_gpytorch'],\n",
    "             shared_inducing_points=ldgd_settings['shared_inducing_points'],\n",
    "             use_shared_kernel=False,\n",
    "             device=device)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T01:42:27.466498200Z",
     "start_time": "2024-02-29T01:42:27.214052Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model_settings = {\n",
    "    'latent_dim': 10,\n",
    "    'num_inducing_points_reg': 100,\n",
    "    'num_inducing_points_cls': 100,\n",
    "    'num_epochs_train': 20000,\n",
    "    'num_epochs_test': 20000,\n",
    "    'batch_size': 700,\n",
    "    'load_trained_model': False,\n",
    "    'load_tested_model': False,\n",
    "    'shared_inducing_points': True,\n",
    "    'use_gpytorch': True,\n",
    "    'random_state': 65,\n",
    "    'test_size': 0.2,\n",
    "    'cls_weight': 1.0,\n",
    "    'reg_weight': 1.0,\n",
    "    'num_samples': 500,\n",
    "\n",
    "}\n",
    "np.random.seed(model_settings['random_state'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T01:43:23.469044400Z",
     "start_time": "2024-02-29T01:43:23.347475600Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# load raw data\n",
    "yn_train, yn_test, ys_train, ys_test, labels_train, labels_test, _ = create_dataset(random_state=model_settings['random_state'], test_size=0.2, dataset='mnist')\n",
    "yn_train = yn_train/yn_train.max()\n",
    "yn_test = yn_test/yn_test.max()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T01:43:24.585748100Z",
     "start_time": "2024-02-29T01:43:24.055685300Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "load_saved_result = False\n",
    "batch_shape = torch.Size([yn_train.shape[-1]])\n",
    "metric_fastldgd_list = []\n",
    "model = create_LDGD_model(data_cont=yn_train, data_cat=ys_train, ldgd_settings=model_settings, batch_shape=batch_shape)\n",
    "\n",
    "if load_saved_result is False:\n",
    "    losses, history_train = model.train_model(yn=yn_train, ys=ys_train,\n",
    "                                              epochs=model_settings['num_epochs_train'],\n",
    "                                              batch_size=model_settings['batch_size'], monitor_mse=False)\n",
    "    model.save_wights(path_save='./saved_models/', file_name=f\"model_mnist_fast\")\n",
    "else:\n",
    "    model.load_weights(path_save='./saved_models/', file_name=f\"model_mnist_fast.pth\")\n",
    "\n",
    "\n",
    "winsound.Beep(freq, duration*3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T02:17:04.669541Z",
     "start_time": "2024-02-29T01:45:43.571548600Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "predictions, metrics, history_test = model.evaluate(yn_test=yn_test, ys_test=labels_test,\n",
    "                                                    epochs=model_settings['num_epochs_test'])\n",
    "winsound.Beep(freq, duration*3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T02:23:28.783480800Z",
     "start_time": "2024-02-29T02:17:04.669541Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\n",
    "alpha_reg = 1 / model.kernel_reg.base_kernel.lengthscale.cpu().detach().numpy()\n",
    "alpha_cls = 1 / model.kernel_cls.base_kernel.lengthscale.cpu().detach().numpy()\n",
    "\n",
    "x = model.x.q_mu.cpu().detach().numpy()\n",
    "std = torch.nn.functional.softplus(model.x.q_log_sigma).cpu().detach().numpy()\n",
    "\n",
    "x_test = model.x_test.q_mu.cpu().detach().numpy()\n",
    "std_test = torch.nn.functional.softplus(model.x_test.q_log_sigma).cpu().detach().numpy()\n",
    "\n",
    "inducing_points = (history_test['z_list_reg'][-1], history_test['z_list_cls'][-1])\n",
    "\n",
    "latent_dim = x.shape[-1]\n",
    "values, indices = torch.topk(torch.tensor(alpha_cls), k=2, largest=True)\n",
    "l1 = indices.numpy().flatten()[0]\n",
    "l2 = indices.numpy().flatten()[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T15:15:23.238420Z",
     "start_time": "2024-02-29T15:15:22.988125200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "color_list = ['r', 'b', 'g', 'c', 'm', 'y', 'k', 'lime', 'navy', 'teal']\n",
    "fig, axs = plt.subplots(2,3, figsize=(32, 20))\n",
    "\n",
    "\n",
    "plot_loss_gplvm(losses, ax=axs[0,0])\n",
    "plot_ARD_gplvm(model_settings['latent_dim'], alpha_cls, ax=axs[0,2])\n",
    "plot_ARD_gplvm(model_settings['latent_dim'], alpha_reg, ax=axs[0,1])\n",
    "plot_scatter_gplvm(x, labels_train, l1=l1, l2=l2, ax=axs[1,0], colors=color_list, show_errorbars=False, std=std)\n",
    "plot_scatter_gplvm(x_test, labels_test, l1=l1, l2=l2, ax=axs[1,1], colors=color_list, show_errorbars=False, std=std_test)\n",
    "\n",
    "plot_heatmap(x, labels_train, model, alpha_cls, cmap='winter', range_scale=1.2,\n",
    "             file_name='latent_heatmap_train', inducing_points=inducing_points, ax1=axs[1,2], fig=fig)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"ARD_synthetic4.png\")\n",
    "fig.savefig(\"ARD_synthetic4.svg\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T15:15:49.412151800Z",
     "start_time": "2024-02-29T15:15:40.214679600Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rec_img, predictions_std = model.regress_x(x_test[1:45])\n",
    "plt.imshow(rec_img.view(-1,28,28).cpu().detach().numpy()[14], cmap='gray')\n",
    "#plt.imshow(predictions_std.view(-1,28,28).cpu().detach().numpy()[5], cmap='gray')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_images = rec_img.size(0)\n",
    "random_indices = np.random.choice(num_images, 9, replace=False)\n",
    "\n",
    "# Create a 3x3 grid of subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "\n",
    "for idx, ax in zip(random_indices, axes.ravel()):\n",
    "    # Convert tensor image to numpy and display it\n",
    "    img = rec_img.view(-1, 28, 28).cpu().detach().numpy()[idx]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')  # Turn off axis\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"generation_mnist.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_images = rec_img.size(0)\n",
    "random_indices = np.random.choice(num_images, 9, replace=False)\n",
    "\n",
    "# Create a 3x3 grid of subplots\n",
    "fig, axes = plt.subplots(1, 9, figsize=(50, 9))\n",
    "\n",
    "for idx, ax in zip(random_indices, axes.ravel()):\n",
    "    # Convert tensor image to numpy and display it\n",
    "    img = rec_img.view(-1, 28, 28).cpu().detach().numpy()[idx]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')  # Turn off axis\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"generation_mnist2.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# For plotting\n",
    "from matplotlib import offsetbox\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n",
    "\n",
    "#For standardising the dat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mnist_train = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "mnist_test = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "x = mnist_train.data.view(mnist_train.data.size(0), -1).numpy()\n",
    "y = mnist_train.targets.numpy()\n",
    "\n",
    "# Concatenate train and test sets to split them later\n",
    "X_test = mnist_test.data.view(mnist_test.data.size(0), -1).numpy()\n",
    "X_test = X_test/X_test.max()\n",
    "y_test = mnist_test.targets.numpy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "standardized_data = StandardScaler().fit_transform(x)\n",
    "print(standardized_data.shape)\n",
    "x_subset = x[0:10000]\n",
    "y_subset = y[0:10000]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tsne = TSNE(random_state = 42, n_components=2,verbose=0, perplexity=40, n_iter=300).fit_transform(x_subset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(tsne[:, 0], tsne[:, 1], s= 5, c=y_subset, cmap='Spectral')\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('Visualizing MNIST through t-SNE', fontsize=24)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_settings = {\n",
    "    'latent_dim': 2,\n",
    "    'num_inducing_points_reg': 10,\n",
    "    'num_inducing_points_cls': 10,\n",
    "    'num_epochs_train': 2000,\n",
    "    'num_epochs_test': 2000,\n",
    "    'batch_size': 1000,\n",
    "    'load_trained_model': False,\n",
    "    'load_tested_model': False,\n",
    "    'shared_inducing_points': True,\n",
    "    'use_gpytorch': True,\n",
    "    'random_state': 65,\n",
    "    'test_size': 0.2,\n",
    "    'cls_weight': 0.0,\n",
    "    'reg_weight': 1.0,\n",
    "\n",
    "}\n",
    "y_one_hot_subset = np.zeros((y_subset.shape[0], len(np.unique(y_subset))))\n",
    "y_one_hot_subset[np.arange(y_subset.shape[0]), y_subset] = 1\n",
    "\n",
    "\n",
    "batch_shape = torch.Size([x_subset.shape[-1]])\n",
    "metric_fastldgd_list = []\n",
    "model = create_LDGD_model(data_cont=torch.tensor(x_subset/x_subset.max()), data_cat=torch.tensor(y_one_hot_subset), ldgd_settings=model_settings, batch_shape=batch_shape, x_init=None)\n",
    "\n",
    "losses, history_train = model.train_model(yn=torch.tensor(x_subset/x_subset.max()), ys=torch.tensor(y_one_hot_subset),\n",
    "                                          epochs=model_settings['num_epochs_train'],\n",
    "                                          batch_size=model_settings['batch_size'], monitor_mse=False)\n",
    "\n",
    "\n",
    "winsound.Beep(freq, duration*3)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "alpha_reg = 1 / model.kernel_reg.base_kernel.lengthscale.cpu().detach().numpy()\n",
    "alpha_cls = 1 / model.kernel_cls.base_kernel.lengthscale.cpu().detach().numpy()\n",
    "\n",
    "latent_mean = model.x.q_mu.cpu().detach().numpy()\n",
    "latent_std = torch.nn.functional.softplus(model.x.q_log_sigma).cpu().detach().numpy()\n",
    "\n",
    "inducing_points = (history_train['z_list_reg'][-1], history_train['z_list_cls'][-1])\n",
    "\n",
    "latent_dim = latent_mean.shape[-1]\n",
    "values, indices = torch.topk(torch.tensor(alpha_cls), k=2, largest=True)\n",
    "l1 = indices.numpy().flatten()[0]\n",
    "l2 = indices.numpy().flatten()[1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(latent_mean[:, l1], latent_mean[:, l2], s= 5, c=y_subset, cmap='Spectral')\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('Visualizing MNIST through t-SNE', fontsize=24)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
